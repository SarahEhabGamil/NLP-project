{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import json\n","import pandas as pd\n","\n","transcription_folder = \"/content/drive/MyDrive/Kareem_Esmail/raw data\"\n","metadata_folder = \"/content/drive/MyDrive/Kareem_Esmail/metadata\"\n","\n","transcription_files = sorted(os.listdir(transcription_folder))\n","metadata_files = sorted(os.listdir(metadata_folder))\n","\n","data = []\n","\n","for txt_file, json_file in zip(transcription_files, metadata_files):\n","    txt_path = os.path.join(transcription_folder, txt_file)\n","    json_path = os.path.join(metadata_folder, json_file)\n","\n","    with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n","        transcription = f.read().strip()\n","\n","    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n","        metadata = json.load(f)\n","\n","    podcast_data = {\n","        \"title\": metadata.get(\"title\", \"Unknown\"),\n","        \"author\": metadata.get(\"author\", \"Unknown\"),\n","        \"categories\": \", \".join(metadata.get(\"categories\", [])),  # Convert list to string\n","        \"keywords\": \", \".join(metadata.get(\"keywords\", [])),  # Convert list to string\n","        \"source\": metadata.get(\"source_url\", \"Unknown\"),\n","        \"publish_date\": metadata.get(\"publish_date\", \"Unknown\"),\n","        \"length\": metadata.get(\"length\", \"Unknown\"),\n","        \"type\": metadata.get(\"type\", \"Unknown\"),\n","        \"transcription\": transcription\n","    }\n","\n","    data.append(podcast_data)\n","\n","df = pd.DataFrame(data)\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","def chunk_by_lines(text, chunk_size=3):\n","    \"\"\"Chunks text based on number of lines.\"\"\"\n","    lines = text.split(\"\\n\")  # Split by newlines\n","    return [\" \".join(lines[i:i+chunk_size]) for i in range(0, len(lines), chunk_size)]\n","\n","def chunk_by_words(text, chunk_size=50):\n","    \"\"\"Chunks text based on number of words.\"\"\"\n","    words = text.split()  # Split by spaces (since no punctuation)\n","    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n","\n","# Apply chunking (choose one)\n","df[\"chunks\"] = df[\"transcription\"].apply(lambda x: chunk_by_lines(x, chunk_size=3))  # Chunk by lines\n","#df[\"chunks\"] = df[\"clean_transcription\"].apply(lambda x: chunk_by_words(x, chunk_size=50))  # Chunk by words\n","\n","# Explode to separate rows\n","df = df.explode(\"chunks\")\n","\n","print(df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","def chunk_by_lines(text, chunk_size=3):\n","    \"\"\"Chunks text based on number of lines.\"\"\"\n","    lines = text.split(\"\\n\")  # Split by newlines\n","    return [\" \".join(lines[i:i+chunk_size]) for i in range(0, len(lines), chunk_size)]\n","\n","def chunk_by_words(text, chunk_size=50):\n","    \"\"\"Chunks text based on number of words.\"\"\"\n","    words = text.split()  # Split by spaces (since no punctuation)\n","    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n","\n","# Apply chunking (choose one)\n","df[\"chunks\"] = df[\"transcription\"].apply(lambda x: chunk_by_lines(x, chunk_size=3))  # Chunk by lines\n","#df[\"chunks\"] = df[\"clean_transcription\"].apply(lambda x: chunk_by_words(x, chunk_size=50))  # Chunk by words\n","\n","# Explode to separate rows\n","df = df.explode(\"chunks\")\n","\n","print(df.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import re\n","\n","def normalize_arabic(text):\n","    text = re.sub(r\"[\\u064B-\\u065F]\", \"\", text)  # Remove diacritics\n","    text = re.sub(r\"[ÿ•ÿ£ÿ¢]\", \"ÿß\", text)  # Normalize Alef variations\n","    text = re.sub(r\"ÿ§\", \"Ÿà\", text)  # Normalize Waw\n","    text = re.sub(r\"ÿ¶\", \"Ÿä\", text)  # Normalize Yeh\n","    text = re.sub(r\"ÿ©\", \"Ÿá\", text)  # Convert Teh Marbuta to Heh\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","from nltk.corpus import stopwords\n","\n","# ‚úÖ Download Arabic stopwords from NLTK\n","nltk.download('stopwords')\n","\n","# ‚úÖ Load Arabic stopwords\n","arabic_stopwords = set(stopwords.words('arabic'))\n","\n","# ‚úÖ Function to remove stopwords\n","def remove_stopwords(text):\n","    words = text.split()\n","    clean_words = [word for word in words if word not in arabic_stopwords]\n","    return \" \".join(clean_words)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(df.head())  # Show first few rows\n","print(df.info())  # Check data types and missing values\n","print(df.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df[\"clean_transcription\"] = df[\"chunks\"].apply(lambda x: remove_stopwords(normalize_arabic(x)))\n","\n","# ‚úÖ Display first cleaned texts\n","df[[\"chunks\", \"clean_transcription\"]].head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Expand each chunk into a separate row\n","df_expanded = df.explode(\"chunks\")\n","\n","# Display first few rows\n","df_expanded.head()\n","# Install openpyxl for Excel export\n","!pip install openpyxl\n","\n","# Save DataFrame to an Excel file\n","df_expanded.to_excel(\"podcast_chunks.xlsx\", index=False)\n","\n","# Download the file in Google Colab\n","from google.colab import files\n","files.download(\"podcast_chunks.xlsx\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import Counter\n","\n","all_words = \" \".join(df[\"clean_transcription\"]).split()\n","word_freq = Counter(all_words)\n","\n","print(word_freq.most_common(20))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","!wget https://noto-website-2.storage.googleapis.com/pkgs/NotoNaskhArabic-unhinted.zip\n","!unzip NotoNaskhArabic-unhinted.zip -d /usr/share/fonts/truetype/\n","wordcloud = WordCloud(font_path=\"/usr/share/fonts/truetype/NotoNaskhArabic-Regular.ttf\", width=800, height=400).generate(\" \".join(df[\"clean_transcription\"]))\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.axis(\"off\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import nltk\n","nltk.download('punkt_tab')\n","from nltk.util import ngrams\n","from nltk import word_tokenize\n","from nltk.util import ngrams\n","from collections import Counter\n","\n","def extract_ngrams(text, n):\n","    tokens = word_tokenize(text)  # Tokenize words\n","    n_grams = list(ngrams(tokens, n))  # Generate n-grams\n","    return n_grams\n","all_text = \" \".join(df[\"clean_transcription\"])  # Combine all podcasts into one string\n","bigrams = extract_ngrams(all_text, 2)\n","trigrams = extract_ngrams(all_text, 3)\n","\n","bigram_counts = Counter(bigrams)\n","trigram_counts = Counter(trigrams)\n","\n","print(\"üîπ Top 10 Bigrams:\")\n","print(bigram_counts.most_common(10))\n","\n","print(\"\\nüîπ Top 10 Trigrams:\")\n","print(trigram_counts.most_common(10))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install arabic-reshaper\n","!pip install python-bidi"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import arabic_reshaper\n","from bidi.algorithm import get_display\n","import matplotlib.pyplot as plt\n","\n","def reshape_arabic(text_list):\n","    return [get_display(arabic_reshaper.reshape(\" \".join(w))) for w in text_list]\n","\n","bigram_words = reshape_arabic([w for w, _ in bigram_counts.most_common(10)])\n","trigram_words = reshape_arabic([w for w, _ in trigram_counts.most_common(10)])\n","\n","bigram_freqs = [freq for _, freq in bigram_counts.most_common(10)]\n","trigram_freqs = [freq for _, freq in trigram_counts.most_common(10)]\n","\n","plt.figure(figsize=(10, 5))\n","plt.barh(bigram_words, bigram_freqs, color='skyblue')\n","plt.xlabel(\"ÿßŸÑÿ™ŸÉÿ±ÿßÿ±\")\n","plt.ylabel(\"ÿ´ŸÜÿßÿ¶Ÿäÿßÿ™ ÿßŸÑŸÉŸÑŸÖÿßÿ™\")\n","plt.title(\"ÿ£ŸÉÿ´ÿ± 10 ÿ´ŸÜÿßÿ¶Ÿäÿßÿ™ ÿ¥ŸäŸàÿπŸãÿß ŸÅŸä ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\")\n","plt.gca().invert_yaxis()\n","plt.gca().invert_xaxis()  # Fix the RTL direction\n","plt.show()\n","\n","plt.figure(figsize=(10, 5))\n","plt.barh(trigram_words, trigram_freqs, color='salmon')\n","plt.xlabel(\"ÿßŸÑÿ™ŸÉÿ±ÿßÿ±\")\n","plt.ylabel(\"ÿ´ŸÑÿßÿ´Ÿäÿßÿ™ ÿßŸÑŸÉŸÑŸÖÿßÿ™\")\n","plt.title(\"ÿ£ŸÉÿ´ÿ± 10 ÿ´ŸÑÿßÿ´Ÿäÿßÿ™ ÿ¥ŸäŸàÿπŸãÿß ŸÅŸä ŸÖÿ¨ŸÖŸàÿπÿ© ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™\")\n","plt.gca().invert_yaxis()\n","plt.gca().invert_xaxis()  # Fix the RTL direction\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(max_features=500)  # Limit to top 500 important words\n","tfidf_matrix = vectorizer.fit_transform(df[\"clean_transcription\"])\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n","\n","# Display the TF-IDF matrix\n","print(tfidf_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install CAMeL Tools with GitHub link as a last resort\n","!pip install git+https://github.com/CAMeL-Lab/camel_tools.git\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import requests\n","import zipfile\n","\n","# Define the URL for the morphology database\n","url = \"https://github.com/CAMeL-Lab/camel-tools-data/releases/download/2022.03.21/morphology_db_calima-msa-r13-0.4.0.zip\"\n","\n","# Define the target directory\n","target_dir = os.path.expanduser(\"~/.camel_tools/data/morphology_db/calima-msa-r13/\")\n","\n","# Create the target directory if it doesn't exist\n","os.makedirs(target_dir, exist_ok=True)\n","\n","# Download the zip file\n","zip_path = \"/tmp/morphology_db_calima-msa-r13.zip\"\n","response = requests.get(url)\n","with open(zip_path, \"wb\") as f:\n","    f.write(response.content)\n","\n","# Extract the zip file into the target directory\n","with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n","    zip_ref.extractall(target_dir)\n","\n","# Clean up the zip file\n","os.remove(zip_path)\n","\n","print(\"Morphology database downloaded and extracted successfully.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from camel_tools.morphology.database import MorphologyDB\n","from camel_tools.morphology.analyzer import Analyzer\n","from camel_tools.utils.dediac import dediac_ar\n","from camel_tools.tokenizers.word import simple_word_tokenize\n","import pandas as pd\n","import re\n","\n","# Load the built-in MorphologyDB for Modern Standard Arabic (MSA)\n","db = MorphologyDB.builtin_db()\n","analyzer = Analyzer(db)\n","\n","def preprocess_arabic_camel(text):\n","    # # Normalize Arabic text\n","    # text = re.sub(r\"[ÿ•ÿ£ÿ¢ÿß]\", \"ÿß\", text)\n","    # text = re.sub(r\"Ÿâ\", \"Ÿä\", text)\n","    # text = re.sub(r\"ÿ©\", \"Ÿá\", text)\n","    # text = re.sub(r\"ÿ§\", \"Ÿà\", text)\n","    # text = re.sub(r\"ÿ¶\", \"Ÿä\", text)\n","    # text = dediac_ar(text)  # Remove diacritics\n","\n","    # Tokenize\n","    tokens = simple_word_tokenize(text)\n","\n","    # Lemmatize using CAMeL Analyzer\n","    lemmas = []\n","    for token in tokens:\n","        analyses = analyzer.analyze(token)\n","        if analyses:\n","            # Extract lemma from 'lex' field, prefer nouns/verbs\n","            lemma = None\n","            for a in analyses:\n","                pos = a.get('pos', '')\n","                lex = a.get('lex', token)\n","                if pos in ['noun', 'verb']:  # Prefer nouns and verbs\n","                    lemma = lex\n","                    break\n","            # Fallback if no noun/verb found\n","            lemma = lemma or analyses[0].get('lex', token)\n","            lemmas.append(lemma)\n","        else:\n","            lemmas.append(token)\n","\n","    return \" \".join(lemmas)\n","\n","# Apply preprocessing\n","df[\"processed_text\"] = df[\"clean_transcription\"].apply(preprocess_arabic_camel)\n","\n","# Preview result\n","print(df[[\"clean_transcription\", \"processed_text\"]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers camel-tools\n","\n","from transformers import AutoTokenizer, AutoModel\n","from camel_tools.morphology.database import MorphologyDB\n","from camel_tools.morphology.analyzer import Analyzer\n","from camel_tools.utils.dediac import dediac_ar\n","import pandas as pd\n","import torch\n","\n","# Load MARBERT Tokenizer and Model\n","marbert_model_name = \"UBC-NLP/MARBERT\"\n","tokenizer = AutoTokenizer.from_pretrained(marbert_model_name)\n","model = AutoModel.from_pretrained(marbert_model_name)\n","\n","# Load CAMeL Tools Analyzer\n","db = MorphologyDB.builtin_db()\n","analyzer = Analyzer(db)\n","\n","def marbert_tokenize(text):\n","    # Tokenize using MARBERT\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=128, padding='max_length')\n","    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","    return tokens\n","\n","def camel_lemmatize(tokens):\n","    # Lemmatize tokens using CAMeL Tools\n","    lemmas = []\n","    for token in tokens:\n","        token = dediac_ar(token)  # Remove diacritics\n","        if token.startswith(\"##\") or token in ['[CLS]', '[SEP]', '[PAD]']:  # Skip subwords and special tokens\n","            continue\n","        analyses = analyzer.analyze(token)\n","        if analyses:\n","            # Pick the first lemma from the analyses\n","            lemma = analyses[0].get('lex', token)\n","            lemmas.append(lemma)\n","        else:\n","            lemmas.append(token)\n","    return lemmas\n","\n","def preprocess_arabic(text):\n","    # Tokenize with MARBERT\n","    tokens = marbert_tokenize(text)\n","    # Lemmatize with CAMeL Tools\n","    lemmas = camel_lemmatize(tokens)\n","    return lemmas\n","\n","# # Sample DataFrame\n","# data = {'text': [\"ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ¨ŸÖŸäŸÑÿ© ŸàŸÖÿπŸÇÿØÿ©.\", \"ÿßŸÑÿ∑ŸÑÿßÿ® ŸäÿØÿ±ÿ≥ŸàŸÜ ŸÅŸä ÿßŸÑÿ¨ÿßŸÖÿπÿ©.\"]}\n","# df = pd.DataFrame(data)\n","\n","# Apply preprocessing\n","df[\"lemmatized_text\"] = df[\"clean_transcription\"].apply(preprocess_arabic)\n","\n","# Preview result\n","print(df[[\"clean_transcription\", \"lemmatized_text\"]])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install transformers sentence-transformers scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","\n","# Load pre-trained Arabic BERT model\n","model = SentenceTransformer(\"aubmindlab/bert-base-arabertv2\")\n","\n","# Convert all chunks to embeddings\n","df[\"embeddings\"] = df[\"chunks\"].apply(lambda x: model.encode(x))\n","df.explode(\"embeddings\").head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(df[\"chunks\"])  # Ensure it's on chunked text\n","\n","# Apply KMeans clustering\n","num_clusters = 7  # Adjust based on your use case\n","kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n","df[\"chunk_cluster\"] = kmeans.fit_predict(X)\n","df = df.explode(\"chunks\")  # Ensure each chunk is in a separate row\n","df = df.dropna(subset=[\"chunks\"])  # Remove empty values if any"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(num_clusters):\n","    print(f\"\\nüîπ Cluster {i} Sample Chunks:\")\n","    print(df[df[\"chunk_cluster\"] == i][\"chunks\"].head(5).tolist())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cluster_mappings = {\n","    0: {\"theme\": \"Inspirational\", \"emotion\": \"Peace\"},\n","    1: {\"theme\": \"Encouraging\", \"emotion\": \"Motivation\"},\n","    2: {\"theme\": \"Reassuring\", \"emotion\": \"Anxiety\"},\n","    3: {\"theme\": \"Reflective\", \"emotion\": \"Calm\"},\n","    4: {\"theme\": \"Challenging\", \"emotion\": \"Self-Doubt\"},\n","    5: {\"theme\": \"Uplifting\", \"emotion\": \"Joy\"},\n","    6: {\"theme\": \"Grounded\", \"emotion\": \"Realism\"}\n","}\n","df[\"theme\"] = df[\"chunk_cluster\"].map(lambda x: cluster_mappings[x][\"theme\"])\n","df[\"emotion\"] = df[\"chunk_cluster\"].map(lambda x: cluster_mappings[x][\"emotion\"])\n","\n","df.explode([\"theme\", \"emotion\"]).head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["theme_summary = df.groupby(\"title\")[\"theme\"].value_counts(normalize=True).unstack()\n","print(\"Theme Summary:\\n\", theme_summary.head())\n","emotion_summary = df.groupby(\"title\")[\"emotion\"].value_counts(normalize=True).unstack()\n","print(\"Emotion Summary:\\n\", emotion_summary.head())\n","summary = df.groupby(\"title\")[[\"theme\", \"emotion\"]].apply(lambda x: x.value_counts(normalize=True)).unstack()\n","print(\"Combined Summary:\\n\", summary.head())\n"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
